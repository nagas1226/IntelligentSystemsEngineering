{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from src.features.preprocess import Preprocessor\n",
    "from src.metrics import rmse\n",
    "from src.config.preprocess import PreprocessorConfig\n",
    "\n",
    "train_df = pl.read_csv(\"../dataset/projectA_vehicle_train.csv\").drop(\n",
    "    \"posting_date\", \"id\"\n",
    ")\n",
    "val_df = pl.read_csv(\"../dataset/projectA_vehicle_val.csv\").drop(\"posting_date\", \"id\")\n",
    "test_df = pl.read_csv(\"../dataset/projectA_vehicle_test.csv\").drop(\"posting_date\", \"id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef50e24",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7303647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the datasets\n",
    "    train_df = pl.read_csv(\"../dataset/projectA_vehicle_train.csv\").drop(\n",
    "        \"posting_date\", \"id\"\n",
    "    )\n",
    "    val_df = pl.read_csv(\"../dataset/projectA_vehicle_val.csv\").drop(\n",
    "        \"posting_date\", \"id\"\n",
    "    )\n",
    "    test_df = pl.read_csv(\"../dataset/projectA_vehicle_test.csv\").drop(\n",
    "        \"posting_date\", \"id\"\n",
    "    )\n",
    "\n",
    "    # data for regression\n",
    "    train_df_reg, val_df_reg, test_df_reg = preprocess_for_regression(\n",
    "        train_df, val_df, test_df\n",
    "    )\n",
    "    price_pred_train, price_pred_val, price_pred_test = run_regression_model(\n",
    "        train_df_reg, val_df_reg, test_df_reg\n",
    "    )\n",
    "\n",
    "    # data for anomaly detection\n",
    "    train_df_anomaly, val_df_anomaly, test_df_anomaly = (\n",
    "        preprocess_for_anomaly_detection(train_df, val_df, test_df)\n",
    "    )\n",
    "    anomaly_pred_train, anomaly_pred_val, anomaly_pred_test = (\n",
    "        run_anomaly_detection_model(train_df_anomaly, val_df_anomaly, test_df_anomaly)\n",
    "    )\n",
    "\n",
    "    constant_prediction = 60_000\n",
    "    alpha = 0.5  # 定数の重み付け係数\n",
    "\n",
    "    ensemble_prediction_train = ensemble_predictions(\n",
    "        price_pred_train, anomaly_pred_train, constant_prediction, alpha\n",
    "    )\n",
    "    ensemble_prediction_val = ensemble_predictions(\n",
    "        price_pred_val, anomaly_pred_val, constant_prediction, alpha\n",
    "    )\n",
    "    ensemble_prediction_test = ensemble_predictions(\n",
    "        price_pred_test, anomaly_pred_test, constant_prediction, alpha\n",
    "    )\n",
    "\n",
    "    # rmse\n",
    "    train_rmse = rmse(\n",
    "        train_df_reg[\"price\"].to_numpy(),\n",
    "        ensemble_prediction_train,\n",
    "    )\n",
    "    val_rmse = rmse(\n",
    "        val_df_reg[\"price\"].to_numpy(),\n",
    "        ensemble_prediction_val,\n",
    "    )\n",
    "    test_rmse = rmse(\n",
    "        test_df_reg[\"price\"].to_numpy(),\n",
    "        ensemble_prediction_test,\n",
    "    )\n",
    "\n",
    "    print(\"======= Ensemble Model =======\")\n",
    "    print(f\"Ensemble Train RMSE: {train_rmse:,.0f}\")\n",
    "    print(f\"Ensemble Validation RMSE: {val_rmse:,.0f}\")\n",
    "    print(f\"Ensemble Test RMSE: {test_rmse:,.0f}\")\n",
    "\n",
    "\n",
    "def preprocess_for_regression(\n",
    "    train_df: pl.DataFrame,\n",
    "    val_df: pl.DataFrame,\n",
    "    test_df: pl.DataFrame,\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    config = PreprocessorConfig()\n",
    "\n",
    "    # 異常検知用に価格の境界を調整（外れ値を除去しない）_config,\n",
    "    config.price_lower_bound = 0\n",
    "    config.price_upper_bound = float(\"inf\")  # 無限大に設定\n",
    "    config.remove_outliers_val = False\n",
    "\n",
    "    # Preprocessorを作成\n",
    "    preprocessor = Preprocessor(**config.to_dict())\n",
    "\n",
    "    train_df_preprocessed, val_df_preprocessed, test_df_preprocessed = preprocessor.run(\n",
    "        train_df, val_df, test_df\n",
    "    )\n",
    "\n",
    "    return train_df_preprocessed, val_df_preprocessed, test_df_preprocessed\n",
    "\n",
    "\n",
    "def preprocess_for_anomaly_detection(\n",
    "    train_df: pl.DataFrame,\n",
    "    val_df: pl.DataFrame,\n",
    "    test_df: pl.DataFrame,\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    config = get_default_config()\n",
    "\n",
    "    # 異常検知用に価格の境界を調整（外れ値を除去しない）_config,\n",
    "    config.price_lower_bound = 0\n",
    "    config.price_upper_bound = float(\"inf\")  # 無限大に設定\n",
    "    config.remove_outliers_val = False\n",
    "\n",
    "    # Preprocessorを作成\n",
    "    preprocessor = Preprocessor(**config.to_dict())\n",
    "\n",
    "    # preprocess the data again\n",
    "    train_df_preprocessed, val_df_preprocessed, test_df_preprocessed = preprocessor.run(\n",
    "        train_df, val_df, test_df\n",
    "    )\n",
    "\n",
    "    # anomaly anotation\n",
    "    anomaly_expr = (pl.col(\"price\") > 40_000).alias(\n",
    "        \"is_anomaly\"\n",
    "    )  # 価格が40,000を超える場合は外れ値とする\n",
    "\n",
    "    train_df_preprocessed = train_df_preprocessed.with_columns(anomaly_expr).drop(\n",
    "        \"price\"\n",
    "    )\n",
    "    val_df_preprocessed = val_df_preprocessed.with_columns(anomaly_expr).drop(\"price\")\n",
    "    test_df_preprocessed = test_df_preprocessed.with_columns(anomaly_expr).drop(\"price\")\n",
    "\n",
    "    return train_df_preprocessed, val_df_preprocessed, test_df_preprocessed\n",
    "\n",
    "\n",
    "def remove_outliers(\n",
    "    df: pl.DataFrame,\n",
    "    lower_bound: float,\n",
    "    upper_bound: float,\n",
    ") -> pl.DataFrame:\n",
    "    return df.filter(\n",
    "        (pl.col(\"price\") >= lower_bound) & (pl.col(\"price\") <= upper_bound)\n",
    "    )\n",
    "\n",
    "\n",
    "def run_regression_model(\n",
    "    train_df: pl.DataFrame,\n",
    "    val_df: pl.DataFrame,\n",
    "    test_df: pl.DataFrame,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    train_df_filtered = remove_outliers(train_df, lower_bound=1_000, upper_bound=40_000)\n",
    "    val_df_filtered = remove_outliers(val_df, lower_bound=1_000, upper_bound=40_000)\n",
    "\n",
    "    # dataset\n",
    "    train_set = lgb.Dataset(\n",
    "        train_df_filtered.drop(\"price\").to_pandas(),\n",
    "        train_df_filtered[\"price\"].to_pandas(),\n",
    "    )\n",
    "    val_set = lgb.Dataset(\n",
    "        val_df_filtered.drop(\"price\").to_pandas(),\n",
    "        val_df_filtered[\"price\"].to_pandas(),\n",
    "        reference=train_set,\n",
    "    )\n",
    "\n",
    "    # モデルの学習\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_leaves\": 31,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 5,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "\n",
    "    # モデルの学習\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_set],\n",
    "    )\n",
    "\n",
    "    # 予測\n",
    "    y_pred_train = model.predict(train_df.drop(\"price\").to_pandas())\n",
    "    y_pred_val = model.predict(val_df.drop(\"price\").to_pandas())\n",
    "    y_pred_test = model.predict(test_df.drop(\"price\").to_pandas())\n",
    "\n",
    "    # 評価\n",
    "    train_rmse = rmse(train_df[\"price\"].to_numpy(), y_pred_train)\n",
    "    val_rmse = rmse(val_df[\"price\"].to_numpy(), y_pred_val)\n",
    "    test_rmse = rmse(test_df[\"price\"].to_numpy(), y_pred_test)\n",
    "    print(\"======= Regression Model =======\")\n",
    "    print(f\"Train RMSE: {train_rmse:,.0f}\")\n",
    "    print(f\"Validation RMSE: {val_rmse:,.0f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:,.0f}\")\n",
    "    print(\"================================\")\n",
    "\n",
    "    return y_pred_train, y_pred_val, y_pred_test\n",
    "\n",
    "\n",
    "def run_anomaly_detection_model(\n",
    "    train_df: pl.DataFrame,\n",
    "    val_df: pl.DataFrame,\n",
    "    test_df: pl.DataFrame,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # モデルの学習\n",
    "    train_set = lgb.Dataset(\n",
    "        train_df.drop(\"is_anomaly\").to_pandas(),\n",
    "        train_df[\"is_anomaly\"].to_pandas(),\n",
    "    )\n",
    "    val_set = lgb.Dataset(\n",
    "        val_df.drop(\"is_anomaly\").to_pandas(),\n",
    "        val_df[\"is_anomaly\"].to_pandas(),\n",
    "        reference=train_set,\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_leaves\": 31,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 5,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_set],\n",
    "    )\n",
    "\n",
    "    # 予測\n",
    "    y_pred_train = model.predict(train_df.drop(\"is_anomaly\").to_pandas())\n",
    "    y_pred_val = model.predict(val_df.drop(\"is_anomaly\").to_pandas())\n",
    "    y_pred_test = model.predict(test_df.drop(\"is_anomaly\").to_pandas())\n",
    "\n",
    "    # 閾値\n",
    "    threshold = 0.2\n",
    "    print(f\"Threshold for anomaly detection: {threshold}\")\n",
    "\n",
    "    # precision\n",
    "    train_precision = precision_score(\n",
    "        train_df[\"is_anomaly\"].to_numpy(),\n",
    "        (y_pred_train > threshold).astype(int),\n",
    "    )\n",
    "    val_precision = precision_score(\n",
    "        val_df[\"is_anomaly\"].to_numpy(),\n",
    "        (y_pred_val > threshold).astype(int),\n",
    "    )\n",
    "    test_precision = precision_score(\n",
    "        test_df[\"is_anomaly\"].to_numpy(),\n",
    "        (y_pred_test > threshold).astype(int),\n",
    "    )\n",
    "    print(\"=============Precision=============\")\n",
    "    print(f\"Train Precision: {train_precision:.2f}\")\n",
    "    print(f\"Validation Precision: {val_precision:.2f}\")\n",
    "    print(f\"Test Precision: {test_precision:.2f}\")\n",
    "\n",
    "    # recall\n",
    "    train_recall = recall_score(\n",
    "        train_df[\"is_anomaly\"].to_numpy(),\n",
    "        (y_pred_train > threshold).astype(int),\n",
    "    )\n",
    "    val_recall = recall_score(\n",
    "        val_df[\"is_anomaly\"].to_numpy(),\n",
    "        (y_pred_val > threshold).astype(int),\n",
    "    )\n",
    "    test_recall = recall_score(\n",
    "        test_df[\"is_anomaly\"].to_numpy(),\n",
    "        (y_pred_test > threshold).astype(int),\n",
    "    )\n",
    "    print(\"=============Recall=============\")\n",
    "    print(f\"Train Recall: {train_recall:.2f}\")\n",
    "    print(f\"Validation Recall: {val_recall:.2f}\")\n",
    "    print(f\"Test Recall: {test_recall:.2f}\")\n",
    "\n",
    "    return y_pred_train, y_pred_val, y_pred_test\n",
    "\n",
    "\n",
    "def ensemble_predictions(\n",
    "    price_predictions: np.ndarray,\n",
    "    anomaly_predictions: np.ndarray,\n",
    "    constant_prediction: float,\n",
    "    alpha: float = 0.9,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    アンサンブル予測を行う関数\n",
    "    :param price_predictions: 価格予測の配列\n",
    "    :param anomaly_predictions: 異常検知の予測値の配列\n",
    "    :param constant_prediction: 定数予測値\n",
    "    :param alpha: 異常検知の重み付け係数\n",
    "    0 <= alpha <= 1\n",
    "    :return: アンサンブル予測の配列\n",
    "    \"\"\"\n",
    "    weight = anomaly_predictions**alpha\n",
    "\n",
    "    ensemble_prediction = (\n",
    "        weight * price_predictions + (1 - weight) * constant_prediction\n",
    "    )\n",
    "\n",
    "    return ensemble_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ccef36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_default_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m test_df = pl.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../dataset/projectA_vehicle_test.csv\u001b[39m\u001b[33m\"\u001b[39m).drop(\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mposting_date\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# data for regression\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m train_df_reg, val_df_reg, test_df_reg = \u001b[43mpreprocess_for_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m price_pred_train, price_pred_val, price_pred_test = run_regression_model(\n\u001b[32m     18\u001b[39m     train_df_reg, val_df_reg, test_df_reg\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# data for anomaly detection\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mpreprocess_for_regression\u001b[39m\u001b[34m(train_df, val_df, test_df)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_for_regression\u001b[39m(\n\u001b[32m     63\u001b[39m     train_df: pl.DataFrame,\n\u001b[32m     64\u001b[39m     val_df: pl.DataFrame,\n\u001b[32m     65\u001b[39m     test_df: pl.DataFrame,\n\u001b[32m     66\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     config = \u001b[43mget_default_config\u001b[49m()\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# 異常検知用に価格の境界を調整（外れ値を除去しない）_config,\u001b[39;00m\n\u001b[32m     70\u001b[39m     config.price_lower_bound = \u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'get_default_config' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896a903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
